{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function # For Python 2 / 3 compatability","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CART Decision Tree\n\n### Dataset used to build the model: ILPD (Indian Liver Patient Dataset) Data Set \n\n### Taken from https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)\n\n### Source of dataset: \n    1. Bendi Venkata Ramana \n    ramana.bendi '@' gmail.com \n    Associate Professor, \n    Department of Information Technology, \n    Aditya Instutute of Technology and Management, \n    Tekkali - 532201, Andhra Pradesh, India. \n\n    2. Prof. M. Surendra Prasad Babu \n    drmsprasadbabu '@' yahoo.co.in \n    Deptartment of Computer Science & Systems Engineering, \n    Andhra University College of Engineering, \n    Visakhapatnam-530 003 Andhra Pradesh, India. \n\n    3. Prof. N. B. Venkateswarlu\n    venkat_ritch '@' yahoo.com \n    Department of Computer Science and Engineering, \n    Aditya Instutute of Technology and Management, \n    Tekkali - 532201, Andhra Pradesh, India.\n\n#### The construction of this CART Algorithm is massively helped by Josh Gordon - https://github.com/random-forests."},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"#### First of all download the dataset from the source stated above and add the data to your notebook. For this particular example, the data (which named liver_patient.csv) will be placed inside a folder named liverpatient and liverpatient is located inside input folder (kaggle's practice). But you can do it your own way too."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"Create the column names and load the dataset with it\"\"\"\n\ncol_names = ['age', 'sex', 'total_bilirubin', 'direct_bilirubin', 'alkaline', 'alamine',\n            'aspartate', 'total_protein', 'albumin', 'A/G Ratio', 'label']\n\ndataset = pd.read_csv(\"../input/liverpatient/liver_patient.csv\", header=None, names=col_names)\n\n\"\"\"Split the dataset into features/attributes and target/label\"\"\"\n\nfeature_cols = ['age', 'sex', 'total_bilirubin', 'direct_bilirubin', 'alkaline', 'alamine',\n            'aspartate', 'total_protein', 'albumin', 'A/G Ratio']\n\nclassification_data = dataset[feature_cols] # Features\nclassification_label = dataset.label # Target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Create array for each dataset.\"\"\"\n\ndataset_array = dataset.to_numpy()\nclass_dataset = classification_data.to_numpy()\nlabel_dataset = classification_label.to_numpy()\n\n\"\"\"Also create a header for the Questions.\"\"\"\n\nheader = ['age', 'sex', 'total_bilirubin', 'direct_bilirubin', 'alkaline', 'alamine', \n          'aspartate', 'total_protein', 'albumin', 'A/G Ratio', 'label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split Dataset into Training and Test Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_be_splitted = pd.read_csv(\"../input/liverpatient/liver_patient.csv\", header=None, names=col_names) #load the dataset\n\ndataset_copy = to_be_splitted.copy()\ntrain_set = dataset_copy.sample(frac=0.60, random_state=0)\ntest_set = dataset_copy.drop(train_set.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Run the code below to see the results of data splitting: the training set and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Set: \")\ndisplay(train_set)\nprint(\"Test Set: \")\ndisplay(test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Effectively we can do that with this code (with Pandas):\ntrain_mean = train_set.mean()\ntest_mean = test_set.mean()\n\nprint(\"Train Set: (Look at the Output) You can see the difference, as NaN values are replaced.\")\ntrain_set.fillna(train_mean).round(3) # Decimal values are rounded to 3 decimal places.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Set: (look at the Output) You can see the difference, as NaN values are replaced.\")\ntest_set.fillna(test_mean).round(3) # Decimal values are rounded to 3 decimal places.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecisionTreeClassifier:\n    \n    \n    def __init__(self, tree):\n        self.tree = DecisionTreeClassifier.build_tree(tree)\n        \n    def unique_vals(rows, col):\n        # Used to find the unique values for \"a\" column in a dataset\n        return set([row[col] for row in rows])\n    \n    def unique_label(rows):\n        # Used to find the unique values for classification_label, note that there is only one column for label. # Rasyid\n        return set([row for row in rows])\n    \n    def class_counts(rows):\n        # Used for dataset array. Returns a dictionary of label -> count.\n        counts = {}\n        for row in rows:\n            # in our dataset format, the label is always the last column\n            label = row[-1]\n            if label not in counts:\n                counts[label] = 0\n            counts[label] += 1\n        return counts\n        \n    def partition(rows, question):\n        # Used to split a dataset into true set and false set.\n        true_rows, false_rows = [], []\n        for row in rows:\n            if question.match(row):\n                true_rows.append(row)\n            else:\n                false_rows.append(row)\n        return true_rows, false_rows\n        \n    def gini(rows):\n        # Used to count the impurity.\n        counts = DecisionTreeClassifier.class_counts(rows)\n        impurity = 1\n        for lbl in counts:\n            prob_of_lbl = counts[lbl] / float(len(rows))\n            impurity -= prob_of_lbl**2\n        return impurity\n    \n    def info_gain(left, right, current_uncertainty):\n        p = float(len(left)) / (len(left) + len(right))\n        return current_uncertainty - p * DecisionTreeClassifier.gini(left) - (1 - p) * DecisionTreeClassifier.gini(right)\n    \n    def find_best_split(rows):\n        best_gain = 0  # keep track of the best information gain\n        best_question = None  # keep train of the feature / value that produced it\n        current_uncertainty = DecisionTreeClassifier.gini(rows)\n        n_features = len(rows[0]) - 1  # number of columns\n\n        for col in range(n_features):  # for each feature\n\n            values = set([row[col] for row in rows])  # unique values in the column\n\n            for val in values:  # for each value\n\n                question = Question(col, val)\n\n                # try splitting the dataset\n                true_rows, false_rows = DecisionTreeClassifier.partition(rows, question)\n\n                # Skip this split if it doesn't divide the dataset.\n                if len(true_rows) == 0 or len(false_rows) == 0:\n                    continue\n\n                # Calculate the information gain from this split\n                gain = DecisionTreeClassifier.info_gain(true_rows, false_rows, current_uncertainty)\n\n                if gain >= best_gain:\n                    best_gain, best_question = gain, question\n        return best_gain, best_question\n    \n    def build_tree(rows):\n        gain, question = DecisionTreeClassifier.find_best_split(rows)\n\n        if gain == 0:\n            return Leaf(rows)\n\n        # If we reach here, we have found a useful feature / value to partition on.\n        true_rows, false_rows = DecisionTreeClassifier.partition(rows, question)\n\n        # Recursively build the true branch.\n        true_branch = DecisionTreeClassifier.build_tree(true_rows)\n\n        # Recursively build the false branch.\n        false_branch = DecisionTreeClassifier.build_tree(false_rows)\n\n        return Decision_Node(question, true_branch, false_branch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Question class: basically the decision node"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Question:\n    \n    \n    # Question Class\n    def __init__(self, column, value):\n        self.column = column\n        self.value = value\n        \n    def is_numeric(value):\n        # To test if a value is numeric.\n        return isinstance(value, int) or isinstance(value, float)\n    \n    def match(self, example):\n        # Compare the feature value in an example to the feature value in this question.\n        val = example[self.column]\n        if Question.is_numeric(val):\n            return val >= self.value\n        else:\n            return val == self.value\n\n    def __repr__(self):\n        # This is just a helper method to print the question in a readable format.\n        condition = \"==\"\n        if Question.is_numeric(self.value):\n            condition = \">=\"\n        return \"Is %s %s %s?\" % (\n            header[self.column], condition, str(self.value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Leaf class for predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Leaf:\n    \n    \n    def __init__(self, rows):\n        self.predictions = DecisionTreeClassifier.class_counts(rows)\n    \n    def print_leaf(counts):\n        total = sum(counts.values()) * 1.0\n        probs = {}\n        for lbl in counts.keys():\n            probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n        return probs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Nodes Class, basically recursively returns every node except leaves."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decision_Node:\n    \n    \n    def __init__(self,\n                 question,\n                 true_branch,\n                 false_branch):\n        self.question = question\n        self.true_branch = true_branch\n        self.false_branch = false_branch\n        \n    def print_tree(node, spacing=\"\"):\n        # Base case: we've reached a leaf\n        if isinstance(node, Leaf):\n            print (spacing + \"Predict\", node.predictions)\n            return\n\n        # Print the question at this node\n        print (spacing + str(node.question))\n\n        # Call this function recursively on the true branch\n        print (spacing + '--> True:')\n        Decision_Node.print_tree(node.true_branch, spacing + \"  \")\n\n        # Call this function recursively on the false branch\n        print (spacing + '--> False:')\n        Decision_Node.print_tree(node.false_branch, spacing + \"  \")\n        \n    def classify(row, node):\n        if isinstance(node,Leaf):\n            return node.predictions\n\n        if node.question.match(row):\n            return Decision_Node.classify(row, node.true_branch)\n        else:\n            return Decision_Node.classify(row, node.false_branch)\n    \n    def testing_result(testing_dataset, tree):\n        for row in testing_dataset:\n            print(\"Actual: %s. Predicted: %s\" %\n                (row[-1], Leaf.print_leaf(Decision_Node.classify(row, tree))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building the tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_tree = DecisionTreeClassifier.build_tree(dataset_array)\nprint(\"Tree:\\n\")\nDecision_Node.print_tree(d_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create the training and test set Decision Tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = train_set.to_numpy()\ntest_set_rows = test_set.to_numpy()\n\nevaluation_tree = DecisionTreeClassifier.build_tree(train_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"Decision_Node.testing_result(test_set_rows, evaluation_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion about this algorithm:\n#### By the results shown, the machine learning model might be overfitted."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}